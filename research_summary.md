Final Report:

# Model Context Protocol (MCP) 技术报告

## 1. 引言

**背景与概述：**Model Context Protocol（MCP，模型上下文协议）是由Anthropic提出的一种开放标准，旨在规范人工智能模型（尤其是大型语言模型，LLM）与外部工具和数据源交互的方式【42:10†source】。它于2024年11月首次发布，并迅速得到主要AI厂商的支持，包括OpenAI和Google DeepMind等【42:10†source】。MCP提供了一个通用接口，使AI能够读取文件、调用函数、处理上下文提示等，从而打破不同系统之间的数据孤岛【42:10†source】。正如官方文档所比喻的那样：“MCP就像AI应用的USB-C接口”——提供了一种标准化方式，将AI模型连接到各种外部数据源和工具 。这一协议的出现标志着AI模型从封闭的认知走向开放互联的重要一步。

**作为函数调用扩展的意义：**在MCP出现之前，开发者已经探索了让LLM执行操作的途径，例如OpenAI于2023年推出的*函数调用*功能，以及ChatGPT插件框架等。这些机制允许模型根据用户请求返回结构化的函数调用，从而由外部代码来执行操作【42:10†source】。然而，此类方案通常是特定厂商绑定的，并缺乏跨平台的统一标准。例如，不同模型厂商定义函数调用格式各异，OpenAI、Anthropic和其他模型输出的函数调用JSON格式都互不兼容【42:3†source】,【42:3†source】。MCP的意义在于作为上述函数调用能力的通用扩展和补充：它关注如何**标准化地执行**这些模型生成的函数调用，并在各类系统中高效、一致地返回结果【42:3†source】,。简单来说，函数调用解决的是“让模型下单”，而MCP负责“让外部系统把订单执行好”。通过MCP，AI模型的指令能够在统一框架下被各类工具可靠执行，避免以往每个模型/工具组合都要定制集成的麻烦【42:4†source】。这使得LLM驱动的应用更具可扩展性和兼容性，在更广泛的环境中发挥作用。

## 2. 历史背景

**起源和动机：**随着大模型应用场景的增多，AI需要访问的外部数据源和工具类型也日趋丰富。然而在MCP之前，开发者经常不得不为每个模型与每个工具分别编写定制的集成代码，形成繁琐的“N×M”对接难题【42:10†source】。Anthropic在2024年倡导MCP开放标准，正是为了解决这一*集成碎片化*难题【42:10†source】。MCP试图将复杂的多对多集成关系简化为“M+N”，即通过一个通用协议层，让M个模型和N个工具可以自由组合，而不需要M×N种适配【42:9†source】。在此之前，OpenAI的函数调用API和ChatGPT插件等虽然一定程度上解决了模型调用外部工具的问题，但这些方案各自为政，需要针对每个提供商单独实现，缺乏持续的会话和统一的安全控制【42:10†source】。MCP的提出顺应了AI应用对**长期上下文**和**即时工具使用**的迫切需求，被视为AI领域类似于语言服务器协议（LSP）的突破：正如LSP统一了各种IDE对编程语言的支持方式，MCP旨在统一各AI应用对外部工具和数据的接入方式【42:2†source】。总的来说，MCP的动机在于解决**无状态交互**、**安全控制分散**以及**多轮上下文缺失**这三大瓶颈，通过引入会话导向的JSON-RPC框架提供富上下文、多轮次的交互能力。

**技术发展中的定位：**MCP的发布标志着AI系统从专有集成走向开放生态的重要里程碑。Anthropic作为发起者在Claude助手中率先支持本地MCP服务器，用于安全访问用户文件和系统工具【42:10†source】。随后诸多公司加入生态：如区块(Block)公司在内部工具中集成MCP以检索专有文档【42:10†source】，开发者社区也积极构建各类MCP服务器。到了2025年3月，OpenAI正式宣布在其产品（包括ChatGPT桌面应用、OpenAI Agents SDK等）中采用MCP标准，将其作为规范AI工具连接的基础【42:11†source】。OpenAI的加入使MCP一跃成为“事实上的”行业标准，紧接着Google DeepMind也确认了即将于新一代Gemini模型及基础设施中支持MCP【42:11†source】。这一连串采用表明业界正趋于在MCP上形成共识【42:11†source】。业界将MCP视为“AI代理时代的开放标准”，其影响相当于Web时代的HTTP或设备互联中的USB标准【42:11†source】, 。可以说，MCP在AI技术发展史上确立了**统一接口**和**互操作**的新范式，为未来各类AI工具生态的繁荣奠定了基础。

**协议结构与工作原理：**MCP采用**客户端-服务器**架构，这一点深受语言服务器协议（LSP）的启发【42:2†source】。架构中涉及三个角色：

- *Host（主机）*：即发起MCP连接的LLM应用或代理环境，例如Claude Desktop应用、VS Code IDE中的AI助手等【42:7†source】。Host代表了AI与用户交互的应用本身。
- *Client（客户端）*：由Host内部运行的MCP客户端组件，它与远程的MCP服务器保持一对一连接【42:7†source】。可以把Client视为Host应用中负责MCP通信的“适配层”。
- *Server（服务器）*：提供上下文和工具的服务端，实现MCP协议规范，向Client暴露功能接口【42:7†source】。Server可以是本地进程（例如对接本地文件系统）或远程服务（例如云端数据库接口）。

这种分层架构确保了AI应用能够通过Client与任意符合协议的Server通信，同时将安全策略与上下文管理职责隔离在不同层次。MCP使用**JSON-RPC 2.0**作为消息基础格式，在Client与Server之间建立起**有状态的会话连接**【42:2†source】。与传统REST API不同，JSON-RPC允许双向通信和请求-响应对应，从而支持复杂的交互模式和持续的对话上下文。

**主要组件和能力：**协议规定了多种消息类型与子协议，以支持不同功能【42:2†source】。Server端可以提供三大类“能力”（capabilities）：

- *Resources（资源）*：即静态或只读的上下文数据，如文件内容、数据库查询结果等【42:2†source】。资源通常以URI标识，可供模型读取，用于丰富上下文。
- *Prompts（提示模板）*：预定义的可重用提示或多步骤对话模板，帮助用户或模型快捷地执行某类任务【42:2†source】。用户可以通过命令或UI触发这些模板，由服务器提供相应的对话内容。
- *Tools（工具）*：服务器暴露的可执行操作，相当于函数或API接口【42:2†source】。通过工具，LLM代理可以执行诸如计算、调用外部API、修改数据等操作，实现对外部世界的影响。

相应地，Client（宿主应用一侧）也可以向Server提供一些能力信息（取决于Host支持），例如：

- *Sampling（取样）*：允许Server请求Host执行一次模型推理，从而实现递归的LLM调用或代理行为【42:2†source】。
- *Roots（根目录）*：Host可告知Server关于用户工作空间、文件系统边界等信息，使Server的操作限定在许可范围内【42:2†source】。
- *Elicitation（提问）*：Server在需要额外信息时，可以请求Host转而询问用户输入（如获取缺失的参数）【42:2†source】。

通过上述能力的双向协商，MCP建立了一个灵活的**扩展机制**：Host和Server在初始化连接时会交换各自支持的功能集，即**能力协商**【42:8†source】。比如，Client在 `initialize`初始化请求中声明自己支持的协议版本和功能（如是否支持Sampling），Server回应它能提供的服务（如支持哪些工具、资源等），然后Client再发送一个 `initialized`通知确认完成握手【42:8†source】。这一三步握手完成后，进入正常通信阶段。在会话中，Client和Server均可发送**请求**（Request）等待对方回复，或发送**通知**（Notification）单向地告知事件【42:8†source】。典型情况下，多数工具调用由Client发起请求、Server执行并返回结果；但Server也可在需要时发起请求（例如要求Host提供额外信息或执行模型推理）【42:8†source】。整个通信过程通过唯一的请求ID来对应请求和响应，并采用标准的JSON结构封装消息，包括方法名（method）、参数（params）、结果（result）或错误（error）等字段【42:7†source】。

**数据流与函数调用的结合：**MCP的数据流可以用一个典型场景来说明：用户在AI应用中提出一个复杂请求，需要检索外部数据并执行操作。首先，LLM通过自身的推理决定需要调用某个工具（比如 `get_current_stock_price`）才能完成任务。在支持函数调用的模型中，它会产生日志式的函数调用输出，如函数名及所需参数【42:3†source】。Host应用拦截到模型的这一输出后，不直接在本地处理，而是利用MCP协议将其转换为相应的请求发送给目标MCP服务器。例如，Host构造一条JSON-RPC请求：方法是 `tools/call`，参数中指定工具名称 `get_current_stock_price`及参数（公司代号AAPL、货币USD），然后通过MCP Client发送给Server。MCP服务器收到请求后，在其内部匹配到对应的工具实现，并执行实际的操作（比如调用股票API获取价格）【42:9†source】。执行完毕后，Server将**结构化的结果**（如股票价格数据）封装在JSON-RPC响应的 `result`字段中返回给Client。Host端的MCP Client收到结果后，交由LLM应用处理——可能是直接以文本形式插入模型回复，或者将数据再作为新消息提供给LLM，促使其生成最终答案。整个过程中，MCP充当了**桥梁**：模型的自然语言请求→函数调用意图，由MCP负责在不同系统之间**发现工具**、**调用执行**并**返回结果**。值得注意的是，MCP允许会话中的多步交互：模型可以根据一次工具结果提出下一个工具调用，从而形成一个链式的推理执行过程。在任何一步调用工具前，Host应用通常会征求用户确认（尤其当工具可能对外部系统有改动作用时），以确保安全【42:2†source】。可以说，MCP数据流紧密结合了函数调用机制：**函数调用是触发点，MCP则提供了完整的调用上下文和执行通道**。通过这一通道，模型的能力被显著拓展，可以动态获取信息和采取行动，而用户和开发者无需为每种模型/工具组合单独编排逻辑，大大降低了复杂度【42:4†source】。

## 3. 技术实现细节

**实现机制：**MCP在实现上采用**JSON-RPC 2.0**作为通信协议基础，这意味着所有消息都以JSON格式封装，遵循远程过程调用（RPC）的请求-响应模式【42:2†source】。这一设计使MCP具备高度的语言无关性和可扩展性：任何支持JSON解析和网络通信的语言都可以实现MCP协议栈。官方提供了多个语言的SDK（如Python、TypeScript、C#、Java等）来方便开发者搭建MCP客户端或服务器【42:11†source】。例如，开发者使用TypeScript SDK可以迅速定义一套MCP Server：指定服务器名称和版本，声明其提供的能力（如有哪些工具或资源），然后调用 `setRequestHandler`设置各类请求的处理函数，最后通过选定的传输方式开始监听【42:8†source】,【42:8†source】。下面展示了一个精简的示例（使用TypeScript）来实现一个MCP服务器，该服务器提供了一个简单的资源列表功能：

```typescript
const server = new Server({ name: "example-server", version: "1.0.0" }, { capabilities: { resources: {} } });
// 注册“列出资源”的请求处理器
server.setRequestHandler(ListResourcesRequestSchema, async () => {
    return { resources: [ { uri: "example://resource", name: "Example Resource" } ] };
});
// 通过stdio启动本地MCP服务器监听
const transport = new StdioServerTransport();
await server.connect(transport);
```
【42:8†source】

如上所示，SDK封装了底层协议细节，开发者只需关注**声明接口**和**实现功能**。无论使用何种语言（Python、C#等SDK类似），编写一个MCP服务器的基本步骤都是：定义服务元数据和能力 -> 注册具体请求/通知的处理逻辑 -> 启动传输层等待连接。对于客户端，实现也类似：通过SDK提供的Client类去建立连接、发送请求以及处理来自Server的响应和通知等。值得一提的是，MCP规范基于TypeScript定义了完整的**JSON模式（schema）**，严格规定了各类请求和数据结构。这使不同实现之间能够保持一致，同时利用JSON-Schema进行参数验证和类型约束。例如，每个工具的输入参数格式用JSON-Schema明确定义，这既可以在Server端用于校验传入参数，也可在客户端用于提示LLM如何构造正确的调用。这种利用模式的严格定义提高了交互的可靠性，减少了因参数不匹配导致错误的情况。

**技术栈与关键组件：**MCP协议栈可以分为**协议层**和**传输层**两部分【42:7†source】,【42:7†source】。协议层负责处理消息格式、请求响应关联、处理过程调度等高层逻辑。例如，SDK中的Protocol或Session类承担了解析JSON消息、调用相应处理器以及发送响应的功能【42:7†source】,【42:7†source】。开发者通过提供handler函数给Protocol类来定义在收到某类请求时如何处理【42:7†source】。传输层则负责底层的数据传递通道，MCP支持多种传输机制来适应不同部署环境【42:7†source】：

- *标准输入/输出（Stdio）通信*：利用本地进程的stdin/stdout进行读写。这种方式非常适合本地运行的MCP服务器（如一个本地工具进程），因其简单高效【42:7†source】。许多桌面AI应用（如VS Code的Agent模式、Claude Desktop等）都支持通过启动一个本地进程并用stdio管道与之通信来接入MCP服务器。Stdio传输的优点是低延迟、实现简单，缺点是Server需要在本地运行且与Host在同一环境。
- *HTTP通信*：MCP也可以通过HTTP进行远程通信，支持使用HTTP POST发送请求、使用Server-Sent Events (SSE)或流式HTTP获取响应流。早期MCP的远程模式主要依赖SSE保持长连接以推送实时消息，但SSE要求服务器长期在线，这对无服务器（serverless）环境支持不足。为此，规范引入了**可流式HTTP (Streamable HTTP)**方案，使MCP服务器可通过常规HTTP请求/响应实现双向流数据传输，从而适应云函数等无状态部署。目前越来越多应用（如Cursor、Claude Desktop等）已采用HTTP模式来注册云端MCP服务。HTTP传输的优势是部署灵活、易于共享（只需提供URL），开发者可以将MCP服务托管在Vercel、Cloudflare等平台并供他人访问。
- *其他传输*：除了上述，MCP理论上也可通过WebSocket、TCP套接字等实现，只要能承载JSON-RPC消息即可。实际上，Server端和Client端在建立连接时会相互沟通所使用的传输和协议版本，以确保兼容【42:8†source】。所有传输通道最终承载的都是JSON-RPC格式的请求、响应和通知【42:7†source】。

**数据格式与处理：**MCP消息采用统一的JSON格式，关键字段包括：`jsonrpc`（协议版本标识）、`id`（请求ID）、`method`（方法名称，如`tools/list`、`tools/call`等）、`params`（参数对象）。一次完整的交互通常为：

- **请求**（Request）：由发送方发起，包含唯一ID、方法名和参数【42:7†source】。如Client请求server列出可用工具：`{ "id":1, "method": "tools/list", "params":{} }`。
- **响应**（Response/Result）：由接收方针对请求返回，包含对应的ID和结果数据【42:7†source】。结果数据可以是任意JSON对象，具体格式取决于请求类型。例如列出工具的响应可能是 `{"id":1, "result": { "tools": [ {...}, {...} ] }}` 列出工具列表。
- **错误**（Error）：若请求无法成功处理，返回错误对象，包含错误代码和消息【42:7†source】。MCP预定义了一些通用错误码（如解析错误、无效请求等），应用也可定义自有错误码（-32000以上）【42:8†source】。错误响应与结果互斥，同一ID只会返回其一。
- **通知**（Notification）：单向消息，仅有方法和参数，无ID，接收方不需回复【42:7†source】。例如Client在会话初始化完成后会发送一个`initialized`通知给Server【42:8†source】。

在更高层的应用语义上，各类方法对应不同功能模块。例如：

- `tools/list`：Client请求Server列出其可用的工具清单【42:4†source】。
- `tools/call`：Client请求调用某个工具并传参执行。
- `resources/list`：请求可用资源列表，`resources/read`：读取某个资源内容，等等。
- `prompts/list`：请求可用提示模板列表。
- `sampling/request`：Server请求Host调用一次LLM（利用Host的模型能力）。
- `roots/list`：Server请求Host提供工作空间根目录或允许的路径信息。

所有这些请求和响应的数据格式都在MCP规范中**严格定义**，确保不同实现之间的互操作性【42:2†source】,【42:2†source】。例如，对工具的定义结构，协议使用JSON Schema描述其输入输出类型。典型的工具定义包含名称、描述、输入Schema和可选的注解属性，如是否只读、是否具副作用等,。如下所示是工具定义的主要字段：

```json
{
  "name": "calculate_sum",               // 工具唯一名称
  "description": "Add two numbers",      // 描述
  "inputSchema": {                      // JSON Schema定义输入参数
    "type": "object",
    "properties": {
      "a": { "type": "number" },
      "b": { "type": "number" }
    },
    "required": ["a", "b"]
  },
  "annotations": {                      // 可选注解
    "title": "求和计算",
    "readOnlyHint": true                // 标记该工具不产生副作用
  }
}
```
,

通过这种结构，Client可以在与模型交互时将工具的约束告诉LLM（例如OpenAI函数调用场景下提供的参数模式），模型便能产生符合Schema的参数，从而提高调用成功率。Server在执行工具前也会依据Schema验证参数有效性，并在必要时返回错误提示，确保整个机制的鲁棒性。

**安全与鉴权机制：**考虑到MCP赋予AI强大的操作能力，协议对安全和信任非常重视。MCP并未强制规定具体的权限实现，但提供了**OAuth 2.0**等统一鉴权流程支持，以及强烈建议的用户确认和许可管理原则【42:4†source】,【42:2†source】。具体而言：Host应用在调用任何敏感工具前应获得用户显式许可【42:2†source】；对于需要认证的MCP服务器，Host可通过OAuth流程获取令牌并附加到MCP请求（VS Code等客户端已支持OAuth鉴权接入）【42:1†source】。此外，Host应对MCP服务器加以管理，只连接可信来源，并为每个Server提供隔离的运行环境,。MCP消息本身可以通过传输层安全（如TLS）保障网络安全【42:8†source】。协议规范也提醒开发者注意防范**提示注入**、**越权操作**等风险【42:11†source】。例如，对于Server提供的工具描述，Host不可盲信其文字说明，应基于信任策略决定哪些工具可自动执行【42:2†source】。随着协议的发展，社区也在探讨引入更精细的权限模型和日志审计功能，以满足企业级安全需求【42:6†source】。

综上，MCP在技术实现上采用了成熟的RPC通信范式，结合严格的Schema定义和灵活的传输支持，使其既严谨又不失灵活。开发者可以利用官方SDK快速构建兼容的服务器或客户端，将现有系统功能暴露为MCP工具。而通过统一的JSON-RPC框架，模型与工具之间的交互变得**可结构化、可监控、可持续**，为AI赋能业务逻辑提供了健壮的基础。

## 4. 优势与应用案例

**相较其他方案的优势：**作为一个开放标准，MCP在技术和性能上具备多方面优势：

- **厂商无关的互操作性：**MCP并非隶属某一家AI厂商的私有方案，它对模型和平台是**解耦**的【42:4†source】。只要模型侧的Host支持MCP，不论背后用的是OpenAI、Anthropic还是本地开源模型，都可以接入同样的MCP工具生态。这种统一接口避免了开发者被某个平台锁定，实现了“一处集成，处处可用”的效果。
- **持续会话与上下文：**与一次一调用的REST API不同，MCP维持了一个**有状态会话**。在会话中，AI助手可以多轮调用工具并保留之前的上下文【42:9†source】。例如一个数据库查询结果可以存为资源，供后续步骤参考；多个工具可以串联操作。这样的**多轮次、多工具交互**对复杂任务尤为关键，MCP天生支持这一点，而早期的ChatGPT插件每次调用相互独立，无法共享状态【42:4†source】。
- **标准化的安全与鉴权：**MCP内置对OAuth2等认证流程的支持，实现对工具访问的统一授权管理【42:4†source】。相比之下，以前每个插件或API各自处理认证，容易混乱和不安全。MCP通过标准机制使认证和权限控制更加可控。此外，MCP要求用户知情并确认每个工具操作，强调“**人在回路**”确保安全【42:2†source】。
- **丰富的工具描述和发现：**MCP定义了工具的模式、描述和注解，客户端可以动态**发现**服务器有什么可用工具,。Model可以根据描述自主选择合适的工具来完成任务。这比起以往需要预先硬编码所有可能调用的函数要灵活得多。并且通过标准的`tools/list`，一个AI代理能轻松获得新工具信息，实现**动态扩展**能力,【42:4†source】。
- **高编排性和扩展性：**借助MCP，不同工具的功能可以像乐高块一样组合。多个MCP服务器提供的能力可以被同一AI代理同时利用，而且遵循相同的调用模式【42:5†source】,【42:5†source】。这意味着随着新工具的出现，只要实现MCP接口，AI助手就能立即“学会”使用它们，无需核心代码改动。这种生态扩展的潜力极大，具备**网络效应**：每增加一个兼容工具，整个生态中所有代理的能力都随之增强【42:6†source】。
- **跨平台与可移植：**MCP让AI对接外部系统的方式标准化后，企业可以更灵活地**切换模型或平台**。例如从OpenAI换成别的LLM时，无需重写背后的工具集成逻辑，只需新的Host继续遵循MCP即可。这为多模型部署提供了便利，同时保护了集成投资不因厂商变化而浪费,。
- **开发者效率：**通过MCP的抽象层，开发者不必再为每个工具单独编写对话逻辑或解析模型输出的代码。LangChain等早期工具链需要拦截模型文本来识别动作，而MCP将这些交互转为结构化消息，减少了模糊解析的工作【42:4†source】。统一的错误处理、日志和调试手段也让开发维护更高效【42:8†source】,【42:8†source】。正如一些分析所言：“ChatGPT插件像封闭工具箱里的专用工具，而MCP是一套开放标准的工具库，任意开发者都能取用”【42:4†source】。

尽管MCP引入了一个额外的通信层，可能带来些许性能开销，但相对于通过提示语解析命令的做法，它能够**避免大量不必要的自然语言处理**（减少token占用），并提供明确的结果结构。这使工具调用更**可靠**且**快速**【42:4†source】,【42:5†source】。随着协议和实现的优化（例如批量请求、异步并发等），MCP的性能已足以支撑大部分实时交互场景，其标准化收益远大于微小的序列化开销。

**实际应用场景：**MCP的优势在许多真实案例中得到体现，目前已知和潜在的应用场景包括：

- **软件开发与IDE助手：**这是MCP最早落地的领域之一。在现代IDE（如Visual Studio Code、Zed等）中，引入MCP让AI编码助手可以安全地访问项目上下文并执行开发相关操作【42:10†source】。例如，VS Code的AI代理通过MCP连接多个服务器：文件系统服务器用于读取/搜索项目文件，代码执行服务器用于运行片段，数据库服务器用于查询项目数据库，测试服务器用于生成和运行测试等【42:5†source】,【42:5†source】。当开发者询问“找出这段代码的性能瓶颈并为每个瓶颈创建一个Issue”，AI助手可以通过MCP**编排**一系列操作：分析代码（调用分析工具），获取结果后调用Issue Tracker工具创建ticket，并最终汇总结果呈现给用户【42:5†source】,【42:5†source】。整个流程中的多步操作都通过MCP以统一接口完成，IDE本身并不需要内置具体逻辑。这种可扩展能力也使得像Replit、Sourcegraph等开发平台引入MCP，以提供更强大的实时编程辅助，“实时编程”（vibe coding）成为可能【42:10†source】。

- **企业知识管理与办公自动化：**在企业环境中，内部的AI助理需要访问各类私有数据和业务系统。MCP为此提供了理想方案：企业可以部署内部MCP服务器连接到CRM、知识库、邮件系统等，然后AI助理经由MCP安全地查询或操作这些系统【42:10†source】。例如，一个销售助理AI可以通过MCP访问**内部知识库**提取最新产品文档，调用**数据库MCP服务器**汇总季度销售数据，再通过**日历MCP服务器**帮团队安排会议【42:5†source】,【42:5†source】。当用户说“准备一份本季度销售报告并安排评审会议”时，AI能够在背后整合多个系统的数据与功能完成这一复杂任务【42:5†source】,【42:5†source】。由于MCP服务器支持OAuth等机制，敏感数据的访问权限可由企业IT统一管控，同时所有交互都有日志可循。这大大增强了AI在企业环境下的**实用性和合规性**。目前已有公司（如金融科技公司Block）将MCP整合进内部知识助手，用于查询专有文档和客户信息【42:10†source】。又如在学术研究中，有人将文献管理工具Zotero通过MCP接口提供给AI助手，让模型能检索论文、提取标注、生成综述，为科研工作提供支持【42:10†source】。

- **智能家居与物联网：**MCP的开放性也非常适合构建跨设备的智能场景。在智能家居中，不同设备通常由各自的服务控制，而AI助理需要统筹控制多个子系统。通过为灯光、恒温器、安全摄像头、媒体播放器等分别实现MCP服务器，统一由家庭AI管家来调用，可以简化自动化流程【42:5†source】,【42:5†source】。例如，用户一句“我要睡觉了”，AI助理通过MCP先后调用灯光服务器关闭所有灯，调用HVAC服务器调整空调模式，调用安防服务器启动警戒模式，调用多媒体服务器暂停音乐等【42:5†source】,【42:5†source】。这些动作由不同厂商的设备执行，但在AI看来只是调用了一系列命令，MCP隐藏了底层的差异，提供了**统一的家庭控制接口**。这比单一品牌的智能助手更灵活，也更易扩展新的设备类型。

- **多步骤决策与Agentic工作流：**MCP还支持所谓“Agentic” AI（代理型智能体）的复杂决策流程。代理型AI需要在自主解决复杂问题时，动态调用多个工具并组合其结果。MCP的**可组合性**使得代理AI可以“即兴地”把不同工具拼接起来解决全新问题【42:6†source】。例如，一个科研助理代理接到请求：“调研最近的气候变化论文，汇总主要发现，并为下周会议制作一份报告”。它可以按需调用**网络搜索MCP服务器**获取论文列表，再用**PDF处理服务器**提取内容，用**摘要服务器**生成要点摘要，接着利用**演示文稿服务器**生成幻灯片，最后通过**日历服务器**预约会议展示【42:6†source】,【42:6†source】。这些操作原本跨度多个领域，但MCP提供的统一接口让代理AI像调用本地函数一样组合它们【42:6†source】。由此产生了“涌现能力”——AI可以通过创新式地组合已有工具，完成远超其单次训练所能涵盖的任务【42:6†source】。这种灵活的工作流在应对突发事件、跨领域问题时尤为有用。

- **Web应用与在线服务：**一些在线平台也开始利用MCP增强AI交互的动态性。例如，Wix在其网页构建平台中嵌入MCP服务器，允许AI工具读取和修改网站内容【42:10†source】。这样一来，用户可以通过AI助手实时编辑站点、生成动态内容，而AI实质上是在调用Wix提供的MCP接口来操作网站【42:10†source】。再比如，许多SaaS服务（GitHub、Slack、Stripe等）都有相应的MCP服务器实现，开发者只需“插上”这些服务器，AI模型就能与这些服务交互【42:11†source】。Anthropic维护的开源MCP服务器列表中，已经涵盖了Google Drive、Git、PostgreSQL、Puppeteer爬虫、支付API等众多集成【42:11†source】。这预示着未来**“AI 即服务”**的形态：各种在线服务通过MCP暴露AI可用接口，AI助手通过统一协议调用，实现跨服务的自动化流程。

综上所述，MCP的应用场景极其广泛，从个人开发环境到企业知识系统，再到物联网设备，都能找到用武之地。其核心价值在于**降低集成门槛**、**扩展AI能力边界**。随着越来越多的工具和服务加入MCP生态，我们将看到AI驱动的工作流变得愈发丰富和高效。开发者和用户可以更专注于需求本身，而将繁杂的连接工作交给MCP这个可靠的“中介层”去完成。

## 5. 与其他协议的比较

MCP作为新兴的开放协议，经常被拿来与此前各种连接AI与工具的方法进行对比。下面我们从功能、性能、兼容性等角度，将MCP与其他主要方案进行分析，并探讨MCP相对于这些方案的改进方向和潜力。

**（1）OpenAI 函数调用 vs MCP：**OpenAI的函数调用（Function Calling）是2023年推出的功能，允许开发者在Prompt中预先定义可供模型调用的函数列表，模型在回答过程中可以选择调用函数并以JSON形式输出函数名和参数【42:4†source】。它与MCP的流程在概念上有相似之处：模型决定调用某函数 -> 外部执行函数 -> 将结果反馈给模型【42:4†source】。但它有以下局限【42:4†source】：

- **平台绑定：**函数调用目前特定于OpenAI或Anthropic等各自的平台实现，每家LLM供应商的格式和API略有不同【42:3†source】,【42:3†source】。开发者如果换模型，函数定义和解析逻辑需要调整，没有统一标准。
- **函数预注册：**在每次对话会话开始时，开发者必须**预先**将所有可能用到的函数告诉模型。函数列表在会话中是固定的，无法动态扩展【42:4†source】。这意味着如果需要新增功能，需要重新开启会话注入新的函数定义。
- **无持续连接：**函数调用缺乏一个与函数提供方的持续通信信道。模型每次输出的函数调用都由开发者代码即时执行，调用之间没有内在的会话状态【42:4†source】。函数执行主要靠临时的代码逻辑，协议本身不涉及执行过程的标准化。

相较而言，MCP弥补了这些不足【42:4†source】：

- MCP是**模型无关**的标准，任何支持MCP的客户端都能调用任意MCP工具——实现了真正的“一次集成，随处运行”【42:4†source】。开发者甚至可以在OpenAI函数调用内部实现一个`call_mcp_tool`通用函数，让模型通过这个单一入口调用各种MCP工具【42:4†source】。
- MCP支持**动态工具发现**和**持久会话**。不需要在会话一开始预注入所有函数，客户端可以在需要时调用`tools/list`获取当前可用工具【42:4†source】。而且客户端与服务器间保持连接，可执行多次交互，支持复杂的对话流程。
- MCP将执行过程标准化。通过统一的JSON-RPC协议，工具的调用、参数传输、错误处理都有规范流程，提供了一致的执行和响应模式。这样不同工具提供者都遵循相同协议，避免了函数调用由中间代码“私有管理”带来的不一致。

换句话说，函数调用更偏重**模型端的指令生成**，而MCP关注**对接端的执行与协作**。前者由LLM厂商控制（如OpenAI API决定支持哪些格式），后者由外部系统主导（开发者和平台决定如何实现MCP支持）。两者实际上可以**优势互补**：函数调用让模型学会决定“*何时*”和“*调用什么*”，MCP确保“不论*哪里*都能执行”且“执行过程井井有条”。

下面将两者做一个简要对比：

| 比较维度   | **函数调用**                                                | **MCP（模型上下文协议）**                         |
|------------|-------------------------------------------------------|--------------------------------------------------|
| **定位作用** | 将用户请求转换为结构化的函数调用，供外部执行【42:3†source】 | 标准化函数/工具的执行流程和结果返回 |
| **控制主体** | 由模型提供方（OpenAI、Anthropic等）在模型内部控制 | 由模型外部的集成系统（客户端/服务器）管理 |
| **接口格式** | 各模型厂商格式各异，一般为JSON结构（无统一标准）【42:3†source】,【42:3†source】 | 采用统一的JSON-RPC协议规范（跨平台统一）, |
| **灵活程度** | 函数需预定义，每个模型接入一个函数集，缺乏持久对话状态【42:4†source】 | 工具可动态发现，支持多轮对话和跨工具协作【42:4†source】 |

*表：OpenAI函数调用 vs 模型上下文协议（MCP）的对比【42:3†source】,。*

可以看到，MCP在扩展性和标准化方面更胜一筹。不过函数调用本身也在演进，不同模型间函数接口差异正在缩小，未来不排除出现**统一的函数调用描述标准**来与MCP接轨。此外，对于只需简单扩展少量功能的小型应用，直接用模型原生的函数调用可能实现最快。而MCP更适合**复杂场景和多工具协同**的需求。

**（2）ChatGPT 插件 vs MCP：**ChatGPT插件是在2023年兴起的一种让模型访问第三方服务的方法。每个插件本质上是一套HTTP API（通常以OpenAPI规范描述）和身份验证流程，ChatGPT作为平台根据用户请求决定调用哪个插件的哪个API。相比MCP，插件模式的问题在于：

- 每个插件都是**独立定制**的，需要插件开发者提供OpenAPI描述和处理逻辑【42:4†source】。不同插件接口风格各异，没有统一的交互协议。调用A服务和调用B服务是截然不同的流程，不利于模型通用地学习使用。
- 插件之间**没有持久会话**或统一协调能力。ChatGPT一次只能调用一个插件，一个插件的调用结束后上下文不自动传递给下一个。缺少像MCP那样的统一会话来 orchestrate 多步骤操作【42:4†source】。
- **平台封闭性：**插件只能在ChatGPT或OpenAI的受控环境下运行，开发者自己构建的AI应用无法直接利用这些插件【42:4†source】。这限制了插件生态的使用面，也使其难以成为普适标准。
- **鉴权杂乱：**每个插件需要自行实现OAuth/API Key等认证，用户每用一个插件就要跳一次授权，各插件安全机制不统一【42:4†source】。从用户角度看，体验复杂且不透明。

MCP针对上述痛点做出了改进【42:4†source】。首先，它提供的是**开放标准**，任何AI平台都能支持。一个MCP服务器一旦出现，不仅ChatGPT，其他AI代理（VS Code、Claude等）都可连接使用【42:4†source】。其次，MCP支持**长连接**，服务器和客户端可以在整个会话中反复交互，保持上下文连续【42:4†source】。再次，MCP规定了**统一的认证方式**（如OAuth 2.0）和权限提示，避免各家插件各搞一套【42:4†source】。换句话说，相比“每个插件一个世界”，MCP打造的是**一个开放且持续的工具接口层**。有人形象地说：“ChatGPT插件像封闭工具箱里的专用工具，而MCP是开放标准的工具库，任何AI平台都能使用”【42:4†source】。当然，ChatGPT插件在MCP未出现前起到了探索作用，为某些早期用例提供了即时解决方案。但从长远看，MCP显然提供了**更标准、更通用**的路径。未来，ChatGPT插件的概念可能会融合入MCP生态，比如OpenAI已将Plugin功能逐步转向MCP兼容的实现【42:11†source】。

**（3）Agent框架（如LangChain等） vs MCP：**在高层协议出现前，开发者常用所谓“代理框架”来让LLM执行动作。例如LangChain、AutoGPT等通过在Prompt中约定格式，让模型输出类似`Action: 搜索`这样的指令，框架解析后执行相应工具，再把结果插入后续Prompt【42:4†source】。这些方案本质上是在模型的自然语言对话上**叠加了一层格式套路**，属于“曲线救国”。其局限在于：

- **非标准、非结构化：**每个框架定义自己的Prompt格式和指令集，模型需针对特定格式调教。如果换一个模型或换一套工具，往往需要重新设计提示或微调模型【42:4†source】。执行步骤通过文本嵌入，缺乏明确的边界，很容易因为模型生成不精确导致解析失败或误操作。
- **工具接口不统一：**各代理框架内置的工具接口（如果有）也互不兼容。如有的要求模型输出`Action: tool_name, Input: ...`，有的用JSON，有的用特殊标记。这些都不是模型训练时固有的能力，因此引入了**额外认知负担**，模型可能对格式变化很敏感【42:4†source】。
- **缺少上下文维护：**大多代理框架依赖在Prompt中累积对话记录来维持上下文，没有像MCP这样明确定义的会话状态。对于长链路执行，很容易Prompt长度暴涨且难以管理。

MCP可以看作是对代理框架思路的**正本清源**。它将工具调用从非结构化的文本协议提升为结构化的网络协议【42:4†source】,【42:5†source】。这样一来，模型不需要输出易错的指令字符串，而是通过标准接口调用，**执行逻辑由客户端保障**，模型只需关注决策。工具的定义和交互由MCP统一管理，开发者不再需要为每个工具设计花哨的提示格式【42:4†source】。同时，会话状态、错误处理等通过协议层面解决，取代了Prompt语境堆积。总的来说，相比代理框架的hack方案，MCP提供了**官方的、统一的“动作语言”**，让模型与工具的交互更健壮和可靠【42:4†source】,【42:5†source】。当然，LangChain等框架也在与时俱进，很多已经支持将MCP作为底层执行器，仍然利用其上层抽象来组织任务。未来我们可能看到框架+MCP结合的模式：框架负责**高层策略**，MCP负责**底层执行**，共同构成AI代理的完整解决方案。

**（4）直接API集成 vs MCP：**最原始的方法莫过于**直接解析模型输出并调用API**。很多早期应用会手工检查用户请求或模型回复中的关键词，然后调用预定的API，将结果再硬插回回复中。这种办法对简单任务可行，但一遇到复杂情况就捉襟见肘【42:5†source】：

- 每新增一个API都要写定制代码和规则，**扩展性差**。多个API交织的情况代码会迅速膨胀，不同API的错误处理也各不相同【42:5†source】。
- **脆弱且依赖人工规则：**模型稍微改变输出措辞，解析规则就可能失效。而且要应对各种边缘情况，使规则集十分复杂【42:5†source】。调试这类系统往往困难重重。
- **缺乏标准错误和权限管理：**自己写集成意味着异常处理、鉴权校验都靠开发者逐一实现，容易遗漏安全问题，也无法复用成熟机制【42:5†source】。

MCP的出现正是为了取代这些手工搭积木式的整合【42:5†source】。使用MCP后，每个外部功能由对应的Server实现标准接口，AI应用只需扮演通用客户端角色。一旦某服务拥有MCP服务器，实现一次，就可以被所有客户端复用【42:5†source】。开发者无需关心不同API的底层细节，只按协议与服务器对话即可。例如，以前可能要解析“帮我发封邮件给张三”然后调用SMTP API；现在只要有一个Email MCP Server，AI助手调用它的`send_email`工具并提供参数就行。错误处理也由MCP规范标准化返回，客户端统一解析提示用户【42:7†source】,【42:8†source】。因此，相对直接集成来说，MCP**大幅降低了实现和维护成本**，并提升了系统整体的一致性。直接集成或许在一两个接口的简单项目中还能用，但对于现代复杂AI应用，这种方式难以为继，MCP则提供了可扩展的架构来满足需求。

**改进方向和潜在挑战：**尽管MCP在以上比较中展示了显著优势，但它并非万能，仍有改进空间和挑战需要关注：

- **潜在竞争与标准融合：**MCP不是唯一一个试图标准化AI工具接入的方案。随着AI应用的发展，可能会出现其他类似协议或框架，业界需要时间进行选择和整合【42:6†source】。目前MCP已获广泛支持，但仍需通过更多实践证明其通用性。如果未来出现多个标准并存，如何避免生态碎片化将是一个挑战【42:6†source】。一个可能的方向是各方共同参与标准组织，将MCP纳入更正式的标准轨道，以减少分裂的风险【42:6†source】。
- **性能优化：**由于MCP引入了JSON-RPC层，会比直连API增加一些序列化和通信开销【42:6†source】。对于那些对延迟极其敏感的场景，例如毫秒级响应的交易系统，当前MCP的通用性可能需要进一步优化。未来的改进可能包括更高效的传输编码（如二进制协议）、批量请求减少往返次数，以及本地部署场景的零拷贝数据传输等。随着实现的成熟，这些优化有望降低MCP的性能损耗，使其几乎与定制集成效率相当。
- **模型对协议的原生支持：**目前LLM对MCP的使用仍需要通过客户端将工具定义传给模型，模型再输出函数调用。这一过程如果模型能直接明白MCP语义，将更高效。一些设想是未来的LLM可以直接以MCP协议格式作为输出（跳过中间的文字描述），或者不同模型就函数调用格式达成统一标准并与MCP衔接。这将进一步减少集成摩擦。不过在这方面还需要模型训练和行业协作。
- **安全与治理：**MCP赋能AI以越来越大的行动力，这也引出了更复杂的安全问题。研究者已经指出了如提示注入、工具滥用等可能的攻击向量【42:11†source】。随着MCP进军企业和关键领域，加强权限细粒度控制、动作审计、资源隔离将非常重要【42:6†source】。社区可能需要制定*MCP安全最佳实践*，比如建立可信MCP服务器清单、签名验证工具代码、防止“名字相似”假冒工具等措施【42:11†source】。只有解决了安全问题，MCP才能在更广泛场景下放心使用。
- **多模态扩展：**当前MCP主要聚焦于文本领域的工具和资源。而未来AI将越来越多地处理图像、音频、视频等多模态数据。MCP可以考虑扩展支持这些领域的标准接口。例如，定义图像分析工具的结果格式、视频处理的流式传输等【42:6†source】。一些初步想法包括图像内容识别MCP服务器、文本转语音MCP服务器等【42:6†source】。通过协议扩展，MCP有潜力成为各类AI模型（不仅仅LLM）的统一上下文接口。

综上，MCP在功能和性能上相对于前辈方案有明显改进，正逐渐成为行业共识。但要巩固这一地位，还需在标准统一、性能提升和安全保障等方面继续演进。在不断对比吸收其他方案优点的过程中，MCP本身也将更加完善。

## 6. 总结与展望

MCP的出现为AI系统与外部世界间架起了一座开放的桥梁。通过这一协议，模型不再受限于训练语料，而是能实时获取新知识、调用外部服务并执行复杂任务，同时以统一、安全的方式进行。这种能力上的飞跃，正如业内人士所评价的，“满足了对富上下文、可操作AI助手日益增长的需求”【42:11†source】。从2024年底发布至今，MCP已从概念走向落地，获得了开源社区和产业界的广泛响应。OpenAI和Google DeepMind的加入尤其具有标志意义——这意味着无论是在私有闭源模型还是开源模型领域，大家都承认需要一个共同的“接口层”来标准化AI功能扩展【42:11†source】。可以预见，MCP有望成为未来AI应用的**通用底座**，其地位类似于互联网时代的HTTP协议，成为默认的交互范式。

**未来发展方向：**展望MCP的未来，我们可以从几个方面进行猜想和展望：

- **生态系统扩张：**随着时间推移，MCP生态将呈指数级壮大。一方面，会有更多现有服务推出官方或社区维护的MCP服务器（例如各大云厂商可能提供自家服务的MCP接口）。另一方面，各种AI应用（桌面助手、移动端代理、行业专用AI等）将内置MCP客户端，从而可以利用丰富的工具生态【42:6†source】。这种网络效应将使MCP的价值愈发凸显：每新增一个工具，所有支持MCP的AI立即获益；每新增一个AI客户端，所有MCP工具的用户群也扩大【42:6†source】。我们或许会看到类似应用商店的“MCP工具市场”，方便地发现和接入第三方MCP服务器。

- **标准化与行业协作：**目前MCP由Anthropic倡导并开源，规范在Github上开放修改建议。随着各大公司参与，有可能成立行业联盟或标准组织来共同制定MCP的后续版本【42:6†source】。例如，Khronos或W3C这样的组织可能介入，将MCP纳入正式标准轨道，确保长期中立性和稳定性。此外，不同厂商可能针对特殊需求提出扩展，但通过标准化流程，可以避免各自为政导致的不兼容【42:6†source】。最终，MCP或将成为AI领域被普遍遵循的开放协议，就像USB或Bluetooth之于硬件设备那样。

- **安全与治理增强：**在MCP广泛应用于企业和关键任务场景后，安全会变得尤为重要。未来我们预计在协议和实现层面都会加强安全控制。例如，引入**细粒度权限**模型：允许用户/管理员针对每个工具赋予精确权限（只读/可写/次数限制等）【42:6†source】；添加**审计日志**和**监控**：所有MCP操作都可记录并审查，以符合企业合规要求【42:6†source】。针对已发现的安全问题（提示注入、权限提升等），会有指南和工具来缓解【42:11†source】。Host应用也将提供更明晰的UI来告知用户AI在请求什么数据、执行什么操作，并让用户有随时中止的能力【42:2†source】。通过这些措施，MCP的强大能力将在**可控可管**的框架下释放，打消用户和企业的后顾之忧。

- **多模态与新能力扩展：**当前的MCP主要围绕文本交互，但未来AI应用将涉及多模态。MCP有潜力扩展支持图像、音频、视频领域的上下文接口【42:6†source】。比如，可以制定图像MCP协议，让AI检索和分析图片数据；音频MCP工具，实现语音的输入输出；视频MCP，用于监控分析等【42:6†source】。这些扩展可能以附加规范或模块形式出现。一旦标准确定，相关领域的模型（如图像识别模型、语音模型）也能参与到MCP生态中，统一与其他工具沟通。此外，MCP还可以结合记忆机制，提供长时记忆或知识库接口，让AI拥有“知识记忆”能力 beyond 单次对话。这些都是MCP未来可以探索的方向。

- **性能与优化：**为了应对更大规模和更高实时性的需求，MCP实现上也会不断优化。例如，针对繁忙的服务器可能实现请求多路复用、并行处理等机制；针对网络传输可能引入压缩或更高效的编码格式；针对客户端可能提供缓存策略、预测执行等以减少延迟。微软等公司或通过Azure等平台为MCP提供优化的基础设施支持，比如Azure在Preview中就支持MCP服务器的托管和快速连接。这些努力将使MCP在保证通用性的同时，在**速度和效率**上也足以胜任高要求场景。

**结语：**Model Context Protocol作为一项新兴技术，短时间内展现出了改变AI应用版图的潜力。它将曾经分散零碎的功能调用整合为统一的开放标准，使AI真正成为一个可扩展的平台而非固化的产品。虽然前路仍有安全、标准等挑战需要克服，但从目前的动向看，MCP正朝着正确的方向迅猛发展。随着越来越多的开发者贡献服务器实现、越来越多的产品支持MCP客户端，这一生态正形成良性循环。当我们展望未来，可以设想一个场景：无论你使用哪个AI助手，它都能通过MCP无缝访问海量外部工具，完成各种复杂任务；而无论你开发了哪个新工具，只要遵循MCP标准，就能立即被所有AI助手调用。这样的AI生态将极大拓展智能代理的边界，使之真正成为人类的万用助手。可以预见，MCP所引领的这场“AI能力互联”革命才刚刚开始，其未来发展值得我们持续关注和参与，共同塑造一个开放、智能、协作的AI新世界。【42:11†source】,【42:11†source】

## References
- [A Beginner's Guide to Visually Understanding MCP Architecture](https://snyk.io/articles/a-beginners-guide-to-visually-understanding-mcp-architecture/)
- [Core architecture - Model Context Protocol](https://modelcontextprotocol.io/docs/concepts/architecture)
- [MCP - Protocol Mechanics and Architecture | Pradeep Loganathan's Blog](https://pradeepl.com/blog/model-context-protocol/mcp-protocol-mechanics-and-architecture/)
- [Model Context Protocol Comparison: MCP vs Function Calling, Plugins, APIs](https://www.ikangai.com/model-context-protocol-comparison-mcp-vs-function-calling-plugins-apis/)
- [Model Context Protocol - Wikipedia](https://en.wikipedia.org/wiki/Model_Context_Protocol)
- [Model Context Protocol (MCP) - Anthropic](https://docs.anthropic.com/en/docs/mcp)
- [A Survey on Model Context Protocol: Architecture, State-of-the-art ...](https://www.techrxiv.org/users/913189/articles/1286748-a-survey-on-model-context-protocol-architecture-state-of-the-art-challenges-and-future-directions)
- [Function Calling vs. Model Context Protocol (MCP): What You Need to Know](https://blog.fotiecodes.com/function-calling-vs-model-context-protocol-mcp-what-you-need-to-know-cm88zfwik000108ji0a1d54fc)
```
